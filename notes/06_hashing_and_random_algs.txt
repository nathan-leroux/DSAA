Hashing and Randomized Algorithms

   Relaxing the demand for either always correct or always efficient can lead to useful algorithms
   Randomized algorithms fall into two types:
   -  Las Vegas algorithms
      Always correct, probably fast
      quicksort is an example

   -  Monte Carlo algorithms
      probably correct, always fast
      

   6.1 Probability Review

      cheeky lil recap, nothing new

      6.1.1 Probability

         an experiment is a procedure that yields one of a set of possible results

         a sample space is all possible outcomes

         an event is a subset of the outcomes

      6.1.2 Compound Events and Independence

         The intersection of two sets is elements that appear in both (and)
         The union of two sets is the elements that appear in either (or)

      6.1.3 Conditional Probability

         Conditional probability of A, given B is the probability of the intersection of A and B, divided the probability of B
         If the two are independent, its just the probability of A

      6.1.4 Probability Distributions

         Random variables can be described by there probability density function, (such as a normal distribution).

      6.1.5 Mean and Variance

         mean, standard deviation and variance etc.

      6.1.6 Tossing Coins

         everything is very likely to be within a few standard deviations from the mean.


   6.2 Understanding Balls and Bins

      Ball and bin problems refer to the analogy of throwing x identical balls into x identical buckets at random.
      Regardless of the hash function, you would expect that each bucket would average balls.
      what happens in practice is that the number of balls across each bucket matches a normal distribution.
      the probability of each bucket containing 1 or 0 balls is actually  1 / e, due to the limit of ( (n-1) / n )^n

      6.2.1 The Coupon Collector's Problem

         How many balls would we be required to throw before we expect all bins to contain at least 1 balls?
         a lot more than the number of bins
         the probability for each successive toss is (n-i) / n
         n is the number of bins, i is the number of buckets containing a ball
         roughly evaluates to nlgn, where lgn is known as the harmonic number


   6.3 Why is hashing a Randomized Algorithm

      A good hash function should distribute hashes over a normal distribution
      However, hashes are not random. They are deterministic so values can be looked up again.
      since they are deterministic is is possible to construct a worst case input.

      Random hash functions can be made by adding another mod operation in computing the hash:

      (f(x) mod p) (mod m)

      f(x) is the function responsible for making the large value
      m is determined by the application for the size of the resulting map
      p can be randomised given that it is much smaller than f(x) and much bigger that m

      this allows the hashing function to give randomized guarantees while still having deterministic lookup.


   6.4 Bloom Filters

      A bloom filter is a technique that solves the problem identifying unique items over a huge data set
      an example used is Google scraping for new web documents, The dictionary shouldnt care about keeping track of each one, it only cares whether it has been added yet.
      Also for huge datasets, the speed of the operation becomes important.

      A Bloom filter uses a binary array instead of a pointer array, when an item exists in the set, it is represented as true by a single bit. by using binary, a bloom filter can have 64 times the capacity.

      However, due to the birthday paradox, with a large enough dataset, collisions will still occour.
      This is solved by using multiple random hash functions and flipping multiple bits to add an item. This greatly decreases the chance of collisions.
      when checking to see whether an item exists, all bits indicated from the hash functions must be flipped.
      This is considered a monte carlo random algorithm as it is probably correct but always fast.


   6.5 The Birthday Paradox and Perfect Hashing

      How many people do you need in a room before it is a 50% chance that at least two of the share the same birthday? the answer is only 23.
      This is the Birthday Paradox, it seems wrong at first but it is actually correct.
      back to hashtables, for the i+1 item, the chance of no collision is 
      (m - i) / m , where m is the number of slots.
      to find the probability of no collisions at all you take the product of this series
      Solving this, we begin to expect collisions when m = n^2
      where n is the number of items

      It is possible to construct a hash table without the O(n) worst case search time (even if it is incredibly rare) if all keys are known in advance and dynamically adding and deleting later are not an issue.
      known as perfect hashing, constant time lookup is done by constructing a hashtable with no collisions, and using quadratic space is not required.

      we hash n items into a hashtable of length n
      given the normal distribution, we expect plenty of collisions. however if the length of the resulting lists made is small enough this is fine.
      a second hashtable is then constructed,
      for each list in the first table, len(l)^2 is allocated in the second to avoid collisions
      each list in the first hashtable also stores its allocated index-range and the hash function used for the second 
      finally, each item for each list is hashed a second time to place it in a unique index in the second hashtable with no collisions, if this is not possible, new hash functions are tried for each list until no collisions occur.


   6.6 Minwise Hashing
      
      The Jaccard similarity is a value between 0 and 1 that describes the similarity of two sets. It is measured by the number of intersecting items in the sets, divided by the number of items in the union 
      If we wanted to compare two documents for similarity we could construct a hashtable from the words in D1, the count the number of intersections between D1 and D2.
      However if we had to compare a very large number of documents for similarity, this would be expensive.

      Minwise Hashing allows efficient similarity comparisons by only comparing the 'k' smallest hashcodes (the values that hash to the first few indexes in the table)
      the Jaccard similarity can be accurately estimated just from comparing this small subset.

      For comparing large numbers of documents, only the k smallest hashcodes need to be stored in memory for each document in order to compare, greatly reducing the memory required.


   6.7 Efficient String Matching

      The Rabin-Karp algorithm uses hashing to enable substring matching in linear time.
      Consider a text string 't', and a pattern string 'p' we are trying to match
      first we calculate the hash of the pattern string using:

      H(p) = c1 * b^(m-1) + c2 * b^(m-2) + ... mod d

      where c is the character in p, and m is the length of p
      after the first index is checked which takes quadratic time, successive indexes can be checked in linear time by doing the following:

         hashing the new character being compaired
         subtracting the hash of the old character no longer being compared
         multiplying the remaining hash values by b to re-align in the formula

      false positives may occour due to collisions, however these can be explicitly checked for a trivial amount of time.


   6.8 Primality Testing

      An elementary way of checking if a number is prime is for each i from 2 to root(n), checking whether n/i is an integer, however O(root(n)) is still too large for numbers where primes are useful such as cryptography, numbers such as 2^512, more than atoms in the universe.

      Fermat's little theorem and randomization allow prime checking (not factoring) in constant time.

      a^(n - 1) = 1(mod n)    if n is prime, and a does not divide n

      create 100 random numbers for a between the range of 1 and n-1, if the probability of a resulting in 1(mod n) is less than 1/2, then the result of all residuals being 1 is (1/2)^100, which is incredibly small. if all tests result in a residual of 1, it can be reasonably assumed n is prime. This is a Monte Carlo type of randomized alg, occasionally gives false positives

      * apart from a few exceptions, see Carmichael Numbers


   6.10 Where do Random Numbers Come From?

      Random numbers generated from many programming libraries come from hash functions.
      These are known as linear congruential generators, essentially a random number Rn is generated by hashing the previous random number Rn-1.

      R(n) = (a*R(n-1) + c) mod m

      where a, c, m and R0 are large and carefully selected constants.
      also known as pseudo-random number generators.

      given that these random numbers are not actually random, it is theoretically possible for an adversary to determine the constants and compromise the random numbers, which is a big oopsie for cybersecurity. 
