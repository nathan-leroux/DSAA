Dynamic Programming
   
   Dynamic Programming is a way to search through all possibilities while caching intermediate results to avoid recalculation.
   It can be applied when a problem can be defined recursively and that definition recalculates the same sub-problems many times.
   If the sub-problems are only calculated once, no performance is gained.
   Intermediate results are stored using a results matrix
   Generally applied on combinatorial problems with a 'left to right' ordering.


   10.1 Caching v Computation

      Dynamic programming is essentially a trade-off between memory usage and time.
      To implement, recursive calls are replaced with a data structure lookup.
      The arguments to the recursive call is the key/index for the data structure.
      After initial calculation, lookup of a value is constant time.

      When defining the problem recursively, it is important to specify the order of recursive statement evaluation.
      Also when initialising the lookup matrix, consider what the base case of the recursive function would be.


   10.2 Approximate String Matching

      Great example of an application of dynamic programming.
      Considers the 'cost' of how far apart two strings are.
      What defines the cost can be application specific.
      Changes that can be made to characters in the string are SUBSTITUTE, DELETE and INSERT.
      Each operation can have a different defined cost

      10.2.1 Edit Distance by Recursion

         First step in dynamic programming is defining the problem recursively
         last character in the string must be either matched, subs added or deleted.
         Therefore the problem can be solved by recursively evaluating the last character
         Where D is the minimum difference, P is pattern and T is text, and i, j are the current index of each string respectively.
         The problem is what is the cost for changing Pattern to Text
         For a MATCH or SUBSTITUTE:
            D[i-1, j-1] +1
            Both indexes are incremented.
         For an INSERTION:
            D[i, j+1] +1
            There is an extra character in the text, pattern is not incremented.
         For a DELETION:
            D[i+1, j] +1
            There is an extra character in the pattern, text is not incremented.

         Pseudo Code:
         Check if length of either strings are zero
         Recursive call for each option, evaluating best cost for each choice now.
         Return lowest cost and series of operations.

         Due to three exponential calls, this implementation is slow as shit.


      10.2.2 Edit Distance by Dynamic Programming

         Most important thing to pay attention to in the previous example is how many recursive calls are made with the same arguments?
         Since there is only len(P) * len(S), not as many as the exponential run time.
         In a P*S matrix, best cost and parent is stored.
         When the matrix is initialised, the recursive base cases are stored.

      10.2.3 Reconstructing The Path

         Start at the end state, by following each cell's 'parent' field, the path can be reconstructed.
         the parent field tells us what the operation was depending on where the next cell is.

      10.2.4 Varieties of Edit Distance

         Examples of other functions that can be performed with only minor changes are:

         Substring matching
            Finding where a short pattern best fits within a large body of text, even with misspellings
            starting costs for different positions in text are set to 0, no penalty for different starting spots
            end cell becomes len(S), (minimum cost at j)

         Longest Common Subsequence
            Longest scattered string of characters that still retain their pattern order
            done by removing substitutions, only insertion/deletion to match

         Maximum Monotone Subsequence
            A numeric sequence is monotonically increasing if the ith element is at least as big as the (i-1)th
            Seeks to delete the fewest characters in the text to leave a monotone subsequence


   10.3 Longest Increasing Subsequence

      Using this as another example to implement dynamic programming
      (Identical to Maximum Monotone Subsequence)

      The three generic steps are:
         1. Formulate the answer as a recursive algorithm
         2. Show that the parameters to your recurrence is bound by a (hopefully small) polynomial
         3. Specify an evaluation order so partial results are always generated before you need them

      The 'Longest Increasing Subsequence' is defined as how the text can be reduced so that each element is greater numerically to the last.
      Only Deletion is used.

      1. Formulate the answer as recursive
         What information about partial solutions is important?
         The length of the longest increasing subsequence up until each index in the partial 
         Each additional character requires the value of each previous character and the longest subsequence that ends at that character.

      2. Show that recurrence parameters are bound
         recurrence parameters are bound by the length of the target text

      3. Specify evaluation order
         table values are calculated 0->n, so i-1 will be available when calculating i

      In order to reconstruct the actual sequence (currently we only have the length)
      we save the predecessor that the current extends.
      This can be backtracked from the goal_cell to get the sequence

   
   10.5 Unordered Partition or Subset Sum

      known as the knapsack problem
      given a target integer k and a set of integers S{}
      is there a subset of S that adds up perfectly to k?

      By considering the integers in S as ordered 1->n, represented by i. the problem can be expressed in a way suited to dynamic programming.
      for each integer in S, either it is in the solution or it is not
      If it is, then the problem reduces to S(n-1) adding to k-Sn

      Solves in exponential time due to the larger k, the more bits needed to represent it


   10.7 Ordered Partition Problem

      Problem involves an ordered series of integers and a number of partitions required.
      How do you place the partitions such that the subseries of numbers are as equal as possible?
      Arises often in parallel processing

      For example:
      3 partitions, set: 1 2 3 4 5 6 7 8 9
      1 2 3 4 5 | 6 7 | 8 9

      the last divider can be placed at some i
      the cost of doing this will be the larger of the following:
         the weight of values in the last partition
         the weight of values in the largest partition to the left of the divider (this is the recursive call)
      The boundary conditions are:
         one element is the smallest partition
         the smallest number of partitions is 1 (not partitioned at all)
      The matrix stores the current largest partition weight and the position of the right-most partition


   10.8 Parsing Context-Free Grammars

      Used in compilers when parsing source code.
      A precise description of the syntax is given by 'Context-Free Grammar' rule
      Each rule describes the interpretation of the left hand symbol, as a sequence of righthand symbols
      The right side symbols are either further rules (non-terminals) or strings (terminals) that arn't evaluated further.
      Assumed that rules are in 'Chomsky Normal Form' X -> YZ
      Means that the righthand side is exactly two non-terminals (rules) or exactly one terminal symbol (string)

      Parsing given text S using the ruleset G constructs a 'parse-tree' of rule substitutions.
      S has length n, while the ruleset G has constant size
      Ruleset that makes up a given language won't change no matter how big the program being compiled

      The rule that is applied at the root of the parse tree splits S into two parts
      0->i describes the section captured by Y and i+1->n by Z, where Y and Z are then evaluated, making the problem recursive
      We keep track of whether some slice of S is generated by non-terminal X
      Base case is defined by terminal strings


   10.9 Limitations of Dynamic Programming

      Dynamic Programing can be applied to any problem that obeys the principal of optimality
      This is when partial solutions can be extended optimally given the state after the partial solution.
      Only the concequences of the decisions are needed, not the decisions actually made.

      Dynamic programming is most efficient of well ordered objects
      The running time of a dynamic programming algorithm is based on:
         The number of partial solutions to keep track of.
         How long it takes to calculate each partial solution.
      Normally the size of the state space is the biggest bottleneck

      When objects are not firmly ordered, there is likely an exponential amount of partial solutions.
